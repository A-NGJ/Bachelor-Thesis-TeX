\chapter{Reinforcement Learning}
\label{cha:reinforcement-learning}

Having discussed ANN and CNN it is high time to introduce third and last component used in the vision system for autonomous boat control. Reinforcement learning is a machine learning training method based on reward-penalty system. The agent is put in the environment with a defined \textbf{action space} and \textbf{state space} \ref{sub:mdp}. The agent is rewarded for taking an action which leads closer to desired state and punished with a negative reward for an undesired behaviour. Value of every action is remembered. This programs the agent to seek a long-term and maximum overall reward in order to achieve an optimal solution.

\section{Reinforcement Learning Methods Taxonomy}
\label{sec:classification-of-reinforcement-learning-methods}

Before diving into details of Q-learning algorithm used in the vision system project, let us give a brief overview over different types of RN methods. The notation introduced in this section is used in further parts of the paper.

\subsection{Policy}
\label{sub:policy}

The policy is a strategy used by the agent to achieve desired goal. It dictates which action is going to be taken based on agent's state and the environment. Formally the policy $\pi(s)$, $s\in{S}$, is defined in terms of the \emph{Markov Decision Process} \ref{sub:mdp} to which it refers. 

\subsubsection*{Off-Policy}
\label{sub2:off-poicy}

% TODO: Q-value reference
% TODO: Q-learning referece
In an off-policy algorithm the agent state and the action that will be taken is evaluated regardless of the currently followed policy $\pi$. Q-learning is an example. It updates its Q-values using Q-value of the next state $s'$ and the greedy action $a'$. That is, the return for state-action pairs is estimated assuming that a greedy policy was followed even though the agent is not following the greedy policy.

\subsubsection*{On-Policy}
\label{sub2:on-policy}

On the other hand, an on-policy algorithm evaluates which action agent should take with respect to the policy it follows. For example, SARSA is an algorithm which updates its Q-values using Q-value of the next state $s'$ and the current policy's action $a''$. The return for state-action pairs is estimated assuming that the current policy is followed in the next state.

\subsection{Model}
\label{sub:model}

Model is in simple terms the behaviour of an environment. Based on whether the agent needs to know such model to determine a policy RN algorithms can be classified to two groups. There are two main types of networks regarding the model.

\subsubsection*{Model-Free}
\label{sub2:model-free}

A model-free algorithm does not have to learn the model to evaluate its policy. One of the most common examples is \emph{Q-learning}, but such approach is also used in \emph{Actor-critic} or \emph{Policy gradient} methods which search directly over policy space to find policies that result in a better reward from the environment.

\subsubsection*{Model-Based}
\label{sub2:model-based}

A model based algorithm must know the model of an environment before finding an optimal policy. Therefore a value of the next step can be predicted without actually performing that step. \mbox{AlphaGO} (\ref{sub:intro-games}) is an perfect example.

\subsection{Base}
\label{sub:base}

This \textbf{Value-based} network does not store explicitly the policy. Instead, it keeps track only of a value function. The policy can be derived directly from the value function, which means taking an action which corresponds to the best value.
The opposite approach is taken in \textbf{Policy-based} methods, where the representation of the policy $\pi(s)$ is created explicitly and kept in memory during the learning process. 

\section{Q-learning}
\label{sec:q-learn}

In a classical reinforcement learning action $a$ the agent should take is determined by the value $V(s')$ of the next state. Q-learning approach focuses on the quality of the action $Q(s, a)$ instead. It is an off-policy, model-free algorithm used in many reinforcement learning solutions.

\subsection{Bellman Equation}
\label{sub:bellman-eq}

This is the underlying principle of the Q-learning algorithm, first introduced by \emph{Richard Bellman} \cite{Bellman:DynamicProgramming}.

\begin{equation}
    V(s) = \max_a\left(R_a(s, s') + \gamma V(s') \right)
\label{eq:bellman}
\end{equation}

Where $V(s)$ is the value of a given state, $R(s, a)$ is a reward for taking an action $a$ in a state $s$ and $\gamma$ is a discount factor. The equation is straightforward, all it tells that a value of a given state is assessed based on an action for which the sum of a reward and discounted value of the next state is maximal. The assumption is that the value of the next state must be known. To put it in simpler terms, an agent which wants to know how good (or bad) its current state is, looks around for opportunities. If there is a happy place around, then the value of current state is high. If you were one step from reaching the top of the Mount Everest, you would perhaps be more excited than if you were looking at it on a postcard.

\subsection{Markov Decision Process}
\label{sub:mdp}

Markov Decision Process can be expressed as a tuple $(S, A, P_a, R_a)$, as stated in \emph{Markov Decision Processes: Concepts and Algorithms} \cite{Otterlo2012MarkovDP}:

\begin{itemize}
    \item $S$ is a state space, a set of all possible states of the agents,
    \item $A$ is an action space, a set of all the behaviors agent can take,
    \item $P_a(s, s') = P(s_{t+1} = s' | s_t = s, a_t = a)$ is the probability that the outcome of an action $a$ taken in state $s$ will be state $s'$ at time $t+1$,
    \item $R_a(s, s')$ is the reward received after transition from state $s$ to state $s'$ after taking action $a$.
\end{itemize}

The key concept behind MDP is that the outcome of an action is independent to previous actions and states. It only depends on a current state. The current state $s$ must give sufficient information to make an optimal decision. A policy $\pi^{*}(s)$ which for every state $s$ results in an optimal decision is called \emph{optimal policy} and can be defined as 

\begin{equation}
    V^{\pi^{*}}(s) \ge V^{\pi}(s)
\end{equation}

for every $s \in S$ and all policies $\pi$.

In realty, when agent takes a particular action it does not ensure that it will end in a desired state. Although it has control over all actions, there environment can still affect them in a unexpected way. Taking a boat as an example, propellers speed may be set to make it turn right, but a sudden wave might move the boat left anyway. That is why the transition in MDP is expressed in terms of probability. Having that in mind, we can write the Bellman equation (\ref{eq:bellman}) in terms of \emph{expected value}:

\begin{equation}
    V^{*}(s) = \max_a \sum_{s'} P_a(s, s') \left(R_a(s, s') + \gamma V^{*}(s') \right)
\label{eq:state-value}
\end{equation}

To select an optimal action given the optimal value function $V^{*}$:

\begin{equation}
    \pi^{*}(s) = arg \max_a \sum_{s'} P_a(s, s') \left(R_a(s, s') + \gamma V^{*}(s') \right)
\label{eq:state-value-policy}
\end{equation}

This is a \emph{greedy policy}, since the best action is always chosen based on the value function $V$.

\subsection{Q-value}
\label{sub:q-val}

Q-value can be described as \emph{quality of the action} $a$. The state-action value function $Q$ becomes an decision making indicator instead of state value function $V$. This slight change has considerable consequences. Since there is no longer need to compute the value of the next state, an algorithm becomes independent to the model. That is, the agent can take a step without prior knowledge of the properties of the incoming state. The action-state equation analogous to \ref{eq:state-value}:

\begin{equation}
    Q^{*}(s, a) = \sum_{s'} P_a(s, s') \left(R_a(s, s') + \gamma \max_{a'} Q^{*}(s', a') \right)
\label{eq:q-value}
\end{equation}

The relation between $Q$ and $V$ is given as following:

\begin{equation}
    V^{*} = \max_a \left(Q^{*} (s, a)\right)
\end{equation}

and analogously to \ref{eq:state-value-policy}

\begin{equation}
    \pi^{*}(s) = arg \max_a Q^{*}(s, a)
\end{equation}

The above equation states that the best action is the one that has the highest return from possible states resulting from taking that action.
In other words, the agent chooses an action which leads to a state which gives the most opportunities to progress. It is like deciding between universities to apply for, assuming you are eligible for admission to every, you would perhaps choose the most prestigious one.

\subsection{Exploration-exploitation}
\label{sub:exploration-exploitation}

The ultimate goal of a Q-learn algorithm is to obtain the optimal policy $\pi^{*}$, but the difficulty is that the model of the environment is unknown. In order to collect missing pieces of information, the agent has to \emph{explore} what is the universe that it has been put into. As interesting as it may be, exploration itself will not bring the agent closer towards finding a desired state $s$ and thus learning the optimal policy. The only way to increase a total reward is to \emph{exploit} the current knowledge about best possible actions. However, there may a better, yet unknown action which only can be discovered in the exploration process. Finding the balance between \emph{exploration} and \emph{exploitation} is crucial for every RN algorithm.

The most rudimentary strategy is \emph{$\epsilon$-greedy} policy. Given the $\epsilon \in [0, 1]$, the agent takes its current best action $arg\max_a Q(s, a)$ with a probability $(1 - \epsilon)$ and a randomly selected action $a \in A$ with probability $\epsilon$. There are obviously more sophisticated strategies to balance between exploration and exploitation, but the Deep-Q-learning \ref{sec:deep-q-learn} algorithm used in the final project is based on this one. Therefore there is no need to introduce additional methods.

\subsection{Temporal Difference Learning}
\label{sub:temporal-difference-learning}

This is a heart of Q-learning algorithm. One could think of two approaches of finding the intermediate values $Q(s, a)$ which eventually lead to desired state $s$. First possibility is to wait until the end of an episode and reward or punish actions taken along the path. For a complex model it would require considerable amount of memory. Luckily there exists a more efficient way. Instead of sitting patiently until the very end and then act, the q-value can be updated at every step using a \emph{temporal difference}.

It was well described by \emph {Richard S. Sutton} \cite{SuttonTD} who wrote a well-depicting analogy. Suppose a weatherman wants to predict on each day of the week a weather on the following Sunday. The conventional way would be to compare the actual outcome to each prediction and then adjust each of them. On the contrary, a temporal difference approach would be to compare each day's prediction with that made on the following day. So if 50\% chance of rain is predicted on Monday, and a 75\% chance on Tuesday, a TD method increases predictions for days similar to Monday. For the Q-learning case is adjusted based on the reward and the estimated discounted value of the next state.

Before diving into mathematics, let us simplify equation \ref{eq:q-value} using an alias,

\begin{equation}
    Q(s, a) = \sum_{s'} P_a(s, s') \left(R_a(s, s') + \gamma \max_{a'} Q(s', a') \right)
    \equiv
    R_a(s, s') + \gamma \max_{a'} Q(s', a')
\label{eq:q-learn-alias}
\end{equation}

and introduce a virtual timer $t$ representing each iteration the agent acts.

Before performing an action, there is some $Q(s, a)$ value known for a transition between current state $s$ and the next $s'$. The agent then performs the action and it turns out that the outcome q-value equals \[R_a(s, s') + \gamma \max_{a'} Q(s', a')\]
An cautious reader may notice that it is simply an above equation \ref{eq:q-learn-alias}. The detail that matters is time. First one is just an expectation and the latter a reality. Temporal difference is simply a neat way to express it,

\begin{equation}
    TD = R_a(s, s') + \gamma \max_{a'} Q_{t}(s', a') - Q_{t-1}(s, a)
\label{eq:temporal-difference}
\end{equation}

It is directly used to update the $Q_t(s, a)$ after taking an action,

\begin{equation}
    Q_t(s, a) = Q_{t-1} + \alpha TD_t(s, a)
\end{equation}

The $\alpha$ factor, called a \emph{learning rate}, is used because do not live in a perfect world, unless you are reading it in a distant future where all diseases are gone. The action taken might have been with some likelihood unexpected or random because of stochastic environment. Therefore, instead of replacing the old $Q_{t-1}$ with the new $Q_t(s, a)$ we update it with a pace controlled by $\alpha$.

\vspace{.5cm}

Finally, the Q-learning algorithm can be written as following,

\begin{lstlisting}[language=Python]
    gamma = x
    alpha = y
    init_q_values()
    for e in episodes:
        s = starting_state
        while s != final_state:
            action = choose_action()
            new_state, reward = take_action(action)
            Q(s, a) = Q(s, a) + alpha*TD
            s = new_state
\end{lstlisting}

\newpage

\section{Deep Q-learning}
\label{sec:deep-q-learn}

Q-learning appears to be a great algorithm for simple reinforcement learning tasks. However, it struggles for more complex environments. One of the reasons is that it explores too slowly and learns only from the consequent states, which may by highly correlated. Fortunately, deep neural networks come with support. Their ability to learn complex patterns comes in handy for a decision-making process in a more sophisticated environment. Instead of TD, the neural network is used to update $Q(s, a)$. The architecture of DNN is straightforward. Input layer consists of \emph{observations} which are all the information that an agent retrieves from the environment. Then there are $N$ hidden layers and finally the output layer made of possible q-values $Q_1(s, a_1), Q_2(s, a_2), ..., Q_x(s, a_x)$, where $x$ is a number of available actions.

\vspace{1cm}

\begin{figure}[h]
\centering
\begin{tikzpicture}
\foreach \i in {1, ..., \inputnum}
{
    \node[circle, minimum size = 6mm, draw=black] (Input-\i) at (0, -\i) {};
}

\foreach \i in {1, 2, ..., \hiddennum}
{
    \node[circle, minimum size = 6mm, draw=black, yshift=(\hiddennum-\inputnum)*5mm] (Hidden-\i) at (2.5, -\i) {};
}

\foreach \i in {1, ..., \outputnum}
{
    \node[circle, minimum size = 6mm, draw=black, yshift=(\outputnum-\inputnum)*5mm] (Output-\i) at (5, -\i) {};
}

\foreach \i in {1, ..., \inputnum}
{
    \foreach \j in {1, ..., \hiddennum}
    {
        \draw[->, shorten >=1pt] (Input-\i) -- (Hidden-\j);
    }
}

\foreach \i in {1, ..., \hiddennum}
{
    \foreach \j in {1, ..., \outputnum}
    {
        \draw[->, shorten >=1pt] (Hidden-\i) -- (Output-\j);
    }
}

\foreach \i in {1, ..., \inputnum}
{
    \draw[<-, shorten <=1pt] (Input-\i) -- ++ (-1, 0)
        node[left]{$observation_{\i}$};
}

\foreach \i in {1, ..., \outputnum}
{
    \draw[->, shorten <=1pt] (Output-\i) -- ++(1, 0)
        node[right]{$q_{\i}$ vs $q_{target\i}$};
}
\end{tikzpicture}
\caption{En example of simple Deep-Q-Network}
\label{fig:deep-q-network}
\end{figure}

The network learns through comparison of each $q_1, q_2, ..., q_n$ with \emph{target} q-values. These are the values remembered from previous steps for taking an action $a$ from the same state $s_{past} = s_{current}$. Then loss\footnote{Loss and cost function are used interchangeably in most cases. Precisely a loss function is for a single training example whereas cost function is the average loss over the entire training dataset} (\ref{sec:cost-function}) is calculated and backpropagated through the network to update all the weights and biases. An example of used loss function is \emph{Mean Squared Error} described in \ref{sub2:cost-function-examples}. Then action is selected based on chosen \emph{action selection policy}. The most commonly used are \emph{$\epsilon$-greedy} (\ref{sub:exploration-exploitation}) or \emph{softmax}, but there have been also different policies proposed \cite{AdaptiveEGreedy}.

\subsection{Experience Replay}
\label{sub:experience-replay}

Deep-Q-learning agents constantly update the value function while observing a stream of experience. As most of us, they learn from their mistakes. In their simplest form, agents drop incoming data immediately after a singe update. This is a naive approach though. If one got stuck in an elevator ten times in a row, they would memorize that it is not the brightest idea to enter it the next time, even though it happened to them ten out of thousands elevator rides. Another example of undesired consequence of instant experience replacement would be a chef who is always covered in tears while cutting an onion except that one time that he cut the bottom last\footnote{This is a real life tip}. The possibly useful rare experiences are instantly forgotten and cannot be used in the future.

\emph{Tom Schaul et al.} \cite{schaul2015prioritized} mentions two main issues of learning without an \emph{Experience Replay}:

\begin{itemize}
    \item Strongly correlated parameters update that break i.i.d. assumption of many popular stochastic gradient-based algorithms.
    \item Rapid forgetting of possibly rare experiences that would be useful later on.
\end{itemize}

Experience replay is a remedy for both. An experience memory is simply a buffer which stores last $M$ transitions. $M$ is a size of a memory buffer and transition is expressed as a tuple

\begin{equation}
    \left(s, a, R_a(s, s'), s'\right)
\end{equation}

Every time parameters are updated instead of comparing to the last experience, transitions are \textbf{uniformly} sampled from a memory buffer. Such sample is usually called a \emph{batch} which size depends on the DQN implementation. By learning from the batch of past experiences temporal correlations get broken and rare experiences may be used more than once. Replay memory could be depicted as a sliding window which follows an agent and contains all its valuable memories. 