\chapter{Vision System Project}
\label{cha:vision-system-project}

Deep learning methods has reached in the recent years state-of-the-art results in many applications, e.g. image recognition, speech recognition and synthesis, natural language processing and reinforcement learning. Having in mind the results described in \ref{sub:intro-games} and the outstanding performance of the application of DQN in teaching Autonomous Driving Agent \cite{2020DuckieTown} it appeared that Autonomous Floating Agent (Boat) would be a natural step. The idea was to create a vision system using CNN (\ref{cha:conv}) + RNN (\ref{cha:reinforcement-learning}) + DNN (\ref{cha:dl}) which would be capable of routing the boat between buoys in the simple environment. Due to time and hardware limitations it was impossible test it in the real world. However, the results are promising enough to suppose that with further development it would be possible to use proposed vision system on a physical boat.

\section{Simulation Environment}
\label{sec:simulation-env}

Before any algorithm begin to train an autonomous agent a virtual reality must be created. Virtual for us of course, for the agent it is the only world that is known. The purpose was to find simple enough environment to minimise external factors which could affect the vision system performance and simultaneously complex enough to benchmark it. Additionally, simulation had to be compatible with \emph{Robot Operating System} (ROS) and \emph{OpenAI Gym} used to train proposed DQN. 

\subsection{Gazebo}
\label{sub:gazebo}

The simulation environment chosen for the project was \emph{Gazebo 11}\footnote{https://gazebosim.org/} - an open source robot 3D simulator.
It is used for designing robots, offering ROS integration, performing regression testing with realistic scenarios and, what is most important, testing robotics algorithms. Needless to say, it perfectly satisfies project's prerequisites. Since the main purpose of the Thesis was the vision system, not the simulation environment itself, there has been chosen a \textbf{Virtual RobotX} \cite{bingham19toward} project as a starting point. It is an simulation environment, designed in coordination with RobotX\footnote{https://defenceinnovationnetwork.com/robotx-challenge-2022/} challenge organizers - the international, university-level maritime autonomous robotic systems competition. However, it required several modifications.

First of all, the environment must have followed simplicity principle, hence the \emph{Sydney regatta} map was used. All obstacles were replaced with buoys which created a route for the boat, as in the following image. 

\begin{figure}[h]
    \centering
    \includegraphics[width=16cm]{img/simulation_view.png}
    \caption{Sydney regatta map with prepared track}
    \label{fig:sydney-regatta-map}
\end{figure}

The buoys create a curve to force an autonomous agent to adjust to changeable conditions. If the track was straight, the environment would present significantly less challenge for the Deep Reinforcement Network. It could possibly neglect all the propellers speed except the "float forward" instruction regardless of what images are perceived by cameras. What is more, left-side buoys are green coloured and of slightly different shape than right-side coloured red. This makes it easier for the CNN to recognize correct patterns and subsequently increases the entire vision system performance.

\subsubsection*{Sensors}
\label{sub2:sensors}

The boat running on the ROS is equipped with multiple sensors, but for the purpose of reinforcement learning the following were used:
\begin{itemize}
    \item Front right camera,
    \item front left camera,
    \item odometry.

    Whereas first two are self-explanatory, odometry is the use of motion sensors to determine the robot's change in the position relative to some known position. In other words, it allows to retrieve $(x, y, z)$ coordinates of the boat. This in turn allows to control whether the boat is within the track or if it reached the desired position.  
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=16cm]{img/front_camera_image.png}
    \caption{View from boat's camera}
    \label{fig:camera-view}
\end{figure}

\subsection{OpenAI Gym}
\label{sub:openai-gym}

OpenAI Gym\footnote{https://gym.openai.com/} is a toolkit for developing and comparing reinforcement learning algorithms. It supports multiple environments, from walking agent to playing games like \emph{Pong} or \emph{Pinball}. Running simulation with gym is straightforward. At every step an agent chooses an action, then takes a step in the environment. In return we receive observation for the state $s'$, reward $R_a(s, s')$ and \emph{done} flag, which is a boolean value.

\begin{lstlisting}[language=Python, caption={Example gym environment simulation}, captionpos=b]
    env = gym.make('My-env')
    observaton = env.reset()
    for _ in range(steps):
        action = agent.chose_action()
        observation, reward, done = env.step(action)
        if done:
            observation = env.reset()
    env.close()
\end{lstlisting}

For our use case the custom environment was created using Gazebo and ROS. The python script communicates with boat robot through ROS topics which then performs an action and sends back the new observation through another ROS topic. The reward is computed after each step based on odometry which is also responsible for controlling agent's position.

\begin{table}[h]
    \centering
    \begin{tabular}{||c c c c||}
        \hline
        Topic & ROS Boat &  OpenAI Env & Description \\
        \hline\hline
        left\_thrust\_cmd & S & P & Left thrust \\
        right\_thrust\_cmd & S & P & Right trust \\
        front\_right\_camera & P & S & Right camera image \\
        front\_left\_camera & P & S & Left camera image \\
        odom & P & S & Odometry \\
        \hline
    \end{tabular}
    \caption{List of ROS topics used in custom OpenAI environment.\newline P - publisher, S - subscriber}
    \label{tab:ros-topics}
\end{table}

\subsubsection*{State space}
\label{sub2:state-space}

Despite the boat being placed in a relatively spacious map, its \textbf{state space} was limited to rectangle of shape $\left( (x_{max} - x_{min}) \times (y_{max} - y_{min})\right)$. Furthermore, the boat can only move between left and right buoys of the same pair, which have positions $(x^i_l, y^i_l)$ and $(x^i_r, y^i_r)$ where $i$ is the buoy pair's index, similarly to layer numbering in ANN. Note that $x^i_l = x^i_r = x^i$, because the directions are aligned with coordinate system. When the agent passes pair of buoys

\begin{equation}
    x^i - \psi \leq x_{agent} \leq x^i + \psi
\label{eq:5.1}
\end{equation}

the constraints must be followed,

\begin{equation}
    y^i_l + \psi \leq y_{agent} \leq y^i_r + \psi
\label{eq:5.2}
\end{equation}

where $\psi$ is an offset. Otherwise the boat is considered off-track, which is an illegal space, and \emph{done} flag is set \emph{true}. After the boat passed pair of corresponding buoys, also called a \emph{checkpoint}, it cannot move backwards any further than $x^i - 3$ or the episode is terminated (done). This constraint gives an direction for the agent to follow. Additionally, maximum distance from desired point has been introduced, which cannot be greater than the largest distance of a state space. The distance is calculated as simple \emph{euclidean distance}

\begin{equation}
    d = \sqrt{(x_1 - x_2)^2 + (y_1-y_2)^2}
\end{equation}

The goal of the simulation is to reach the desired point. If $(x_{agent}, y_{agent}) = (x_{desired}, y_{desired})$ then simulation episode finishes and \emph{done} is set \emph{true}. 

\subsubsection*{Action Space}
\label{sub2:action-space}

The boat can take one of eight discrete actions, which are predicted by DQN algorithm.

\vspace{.5cm}

\begin{table}[h]
\centering
\begin{tabular}{||l c c ||}
    \hline
     Action & Left Propeller Speed & Right Propeller Speed \\
     \hline\hline
     0 (Forward) & 1 & 1 \\
     1 (Backward) & -1 & -1 \\
     2 (Left) & 1 & -1 \\
     3 (Right) & -1 & 1 \\
     4 (0.5 Right) & -0.5 & 0.5 \\
     5 (0.5 Left) & 0.5 & -0.5 \\
     6 (0.3 Right) & -0.3 & 0.3 \\
     7 (0.3 Left) & 0.3 & -0.3 \\
    \hline
\end{tabular}
\caption{Discrete actions predicted by the DQN algorithm}
\label{tab:action-space}
\end{table}

\subsubsection*{Observation Space}
\label{sub2:observation-space}

Usually the observation space $O$ is just a part of the entire state space $S$. However in this particular use case the information about the relative $(x, y, z)$ location was replaced by images from cameras. The agent does not have an access to odometry information. The only way to perceive the environment is through its vision system.

The Raw 1280x720 are preprocessed before feeding the neural network. This step is crucial in achieving close to optimal performance, because only particular features on image are useful. In our particular case buoys are what matters. Neither the beautiful blue sky nor eye-catching azure water bring any viable information for keeping the boat inside the track. Therefore the following steps are performed:

\begin{enumerate}
    \item \textbf{Cropping}
    
    The bottom 240 pixels are removed, since they contain only redundant water and front of the boat. To get wider picture refer to fig. \ref{fig:camera-view}.
    
    \item \textbf{Concatenation}
    
    Left half of the left's camera image is concatenated with right half of the right's camera image along vertical axis. Resulting image has the same resolution as original one.
    
    \item \textbf{Color segmentation}
    
    To make it easier for the neural network to recognize important parts of an image the key colors are segmented based on their values. In order to perform that, the mask is applied which set all the pixels which are not red to 0. Pixel color is understood as the output of all RGB channels. Same operation is applied for green, because these are colors of buoys. Finally both masked out images are added resulting in the following image.
    
    \begin{figure}[h]
        \centering
        \includegraphics[width=16cm]{img/image_color_segmentation.png}
        \caption{Image after color segmentation}
        \label{fig:color-segmented-image}
    \end{figure}
    
    \item{Resizing}
    
    Images are downscaled from the original 1280x720 resolution to 80x60. It significantly accelerates the training process while keeping all the valuable information.
    
    \item \textbf{Normalization}

    Pixel values are normalized to $[0, 1]$ again for boosting neural network performance, CNN in particular.
    
    \item \textbf{Image stacking}
    
    The last 5 camera images are concatenated into a tensor with dimensions $(64, 80, 3 \times 5)$. The numbers represent height, width, number of channels and sequence length respectively. This approach creates a time series containing enriched information that results in better policy networks compared to a single image. Same method was used in preprocessing stage by \emph{Google Deep Mind} in a state-of-the-art DQN exceeding human-level performance in Atari2600 games \cite{DQNAtari}.
\end{enumerate}

Consequently, after all preprocessing steps have been performed, the observation space is a $3^{rd}$ order $(64 \times 80 \times 15)$ tensor. Values are within $[0, 1]$ range.

\subsubsection*{Reward}
\label{sub2:reward}

There have been various reward systems tested. At first the agent had been rewarded with small value $R_a(s, s')$ for every step that got it closer to the desired point $(x_d, y_d)$ and punished with $-R_a(s, s')$ otherwise. If it managed to reach $(x_d, y_d)$ it was additionally rewarded with much greater \emph{done reward} $40R_a(s, s')$ and punished with $-40R_a(s, s')$. Although the second reward is undoubtedly necessary to allow the reinforcement network distinguish between successful and unsuccessful episode, the small partial reward was confusing. There were parts of the track which forced the boat to float against the "current". That is in order not to leave the track the agent had to acquire negative rewards throughout several episodes, which was breaking the learning process at some point eventually.

The new approach replaced partial step reward with checkpoint reward. Pairs of corresponding buoys are stored in a LIFO\footnote{Last in first out} list. The agent crosses the checkpoint when both \ref{eq:5.1} \& \ref{eq:5.2} are satisfied and is moving towards desired position. When it occurs, the pair of buoys is popped out from list to prevent the boat from gaining multiple rewards for crossing the same checkpoint. Altogether, the maximum reward equals 

\begin{equation}
R_{max} = N \cdot R_a(s, s') + 40R_a(s, s')
\end{equation}

where $N$ is number of checkpoints.

Such strategy for reward distribution turned out to be effective despite its simplicity.

\section{Deep Reinforcement Network Architectures}
\label{sec:dnn-architectures}

There has been four deep neural network architecture evaluated, all of which base on a DQN described more in depth in section \ref{sec:deep-q-learn}. Each consecutive one was expected to present better performance, because every architecture except the core Deep Q-Network is only an extension which addresses some of know issues of the predecessor. The goal was to compare each network's performance in the same simulation scenario, for an exact set of hyperparameters derived from empirical trials. No grid search was performed owing to high computational cost. The results of such comparison are presented in the following chapter.

\begin{table}
    \centering
    \begin{tabular}{|c c|}
        \hline
        \multicolumn{2}{|c|}{Hyperparameters} \\
        \hline
        learning rate (lr) & $1e^{-5}$ \\
        $\gamma$ & $0.99$ \\
        $\epsilon$ & $1.0$ \\
        $\epsilon$ decrease & $5e^{-5}$ \\
        $\epsilon_{min}$ & $0.01$ \\
        memory size & $4e^4$ \\
        batch size & $32$ \\
        replace target network steps & $1000$ \\
        max steps & $1000$ \\
        \hline
    \end{tabular}
    \caption{Hyperparameters used in every architecture}
    \label{tab:hyperparameters}
\end{table}

\subsection{Deep Q-Network}
\label{sub:deep-q-network}

This is the underlying architecture behind every network in the research. It was highly inspired by \emph{Google Deep Mind} research from 2015 \cite{DQNAtari}, which has been already mentioned in the Thesis. Although this DQN has presented outstanding performance in the Atari2600 environment it had been questionable whether same results would be observed in the Gazebo environment. There are considerably more distinct states which differ only slightly from one another. Unlike the Atari game where screens change more dynamically and contain more characteristic features.

Most of the key concepts behind the implementation of Deep Q-Network, such as \emph{temporal difference}, \emph{q-value}, \emph{experience replay}, \emph{markov decision process} or \emph{bellman equation}, have been already discussed in sections \ref{sec:q-learn} and \ref{sec:deep-q-learn}.
The issue with direct implementation of those is that reinforcement learning is known to be unstable or even to diverge when a nonlinear function approximator such as a neural network is used to represent the action-value (Q) function. These instabilities are addressed by experience replay (\ref{sub:experience-replay}) and splitting evaluation and prediction into \emph{Evaluation Q-Network} (Q-eval) and \emph{Target Q-Network} (Q-Target). Whereas the first one is used to update weights using stochastic gradient descent and a loss function, the latter is used to make predictions about the next action. The action-value function $Q(s, a; \theta_i)$ is parameterized, where $\theta_i$ are the weights of the Q-network at iteration $i$. Target network is an exact copy of the evaluation network having parameters $\theta^-_i$ replaced (copied $\theta_i$ from the evaluation network) every $C$ steps and held fixed between individual updates. This approach significantly stabilises learning process.

Experience replay $e_t = (s_t, a_t, R_{at}(s, s'), s_{t+1})$ is stored at each time step $t$ in a buffer $D_t = \{e_{\max\left(1, (t - mem\_size)\right)}, ..., e_t\}$. During learning there are updates applied on uniformly drawn samples of experience $U(D)$ of size $batch\_size$. The update means backpropagating stochastic gradient descent at iteration $i$ using the following loss

\begin{equation}
    L_i(\theta_i) = \sum_{(s, s') \sim U(D)}P_a(s, s')\left(R_a(s, s') + \gamma \max_{a'} Q(s', a'; \theta^-_i) - Q(s, a; \theta_i) \right)^2
\end{equation}

Which is actually a squared, parameterized temporal difference (\ref{sub:temporal-difference-learning}).

\subsubsection*{Model Architecture}
\label{sub2:model architecture}

The architecture used takes as an input state observation (\ref{sub:openai-gym}/Observation space) and returns predicted Q-values for all possible actions. The greatest advantage of such solution is that only one forward pass is needed to compute Q-value for every action. The input to the neural network consist of 5 stacked $64 \times 80$ images. There are three convolutional layers in the network, each proceeded by ReLU (\ref{sec:activation-function}) and max pool (\ref{sec:conv-pooling}) operations. The convolutional layers use 32, 64, 64 filters of $3 \times 3$ size. The max pool layers use $2 \times 2$ filter reducing the size of input $8$ times to $8 \times 10$. The hidden fully connected layer consist of 256 linear rectifier units. The output layer is a fully-connected linear layer with number of outputs representing number of actions, in our case $8$.

\begin{figure}[h]
    \centering
    \includegraphics[width=18cm]{img/model-architecture.png}
    \caption{DQN network used in the vision system}
    \label{fig:dqn-network}
\end{figure}

\subsubsection*{Algorithm}
\label{sub2:dqn-algorithm}

The algorithm steps are nearly identical to the Q-learning (\ref{sec:q-learn}) with the exception that nonlinear approximator is used instead of a linear one. Therefore optimal action-value function is estimated as $Q(s, a; \theta) \approx Q^*(s, a)$. The neural network approximator with weights $\theta$ is referred as Deep Q-network. It is trained by adjusting weights $\theta_i$ at step $i$ to reduce loss function where target values $y = R_a(s, s') + \gamma \max_{a'}Q^*(s', a')$ are replaced by approximate target values $y = R_a(s, s') + \gamma \max_{a'}Q(s', a';\theta^-_i)$ using weights $\theta^-_i$ from some previous step.

This Deep Q-learning algorithm is also an model-free and off-policy algorithm, just as a regular Q-learning. The agent follows a policy $\pi(s)$ ensuring adequate exploration-exploitation balance while learning about the greedy policy $\pi^*(s) = arg\max_{a'}Q(s, a'; \theta)$.

\subsection{Double Deep Q-Network}
\label{sub:double-deep-q-network}

\subsection{Dueling Deep Q-Network}
\label{sub:dueling-deep-q-network}

\subsection{Dueling Double Deep Q-Network}
\label{sub:dueling-double-deep-q-network}

\subsection{Training details}
\label{sub:training-details}

